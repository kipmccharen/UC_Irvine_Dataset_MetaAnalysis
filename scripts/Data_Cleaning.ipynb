{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data scraped from UC Irvine ML Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pandas package and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#Load data\n",
    "pre_cleaned_df = pd.read_csv('UC_Irvine_ML_datasets.csv')\n",
    "\n",
    "#select only relevant columns\n",
    "pre_cleaned_df = pre_cleaned_df[[\n",
    "                                'header', 'DataSetCharacteristics', 'NumberofInstances', 'Area',\n",
    "                                'AttributeCharacteristics', 'NumberofAttributes', 'DateDonated',\n",
    "                                'AssociatedTasks','MissingValues', 'NumberofWebHits'\n",
    "                                ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN in columns\n",
    "def fillna(df):\n",
    "    df['MissingValues'] = df['MissingValues'].fillna('No')\n",
    "    df['NumberofInstances'] = df['NumberofInstances'].fillna(0)\n",
    "    df['NumberofAttributes'] = df['NumberofAttributes'].fillna(0)\n",
    "    df['AttributeCharacteristics'] = df['AttributeCharacteristics'].fillna('Other')\n",
    "    df['AssociatedTasks'] = df['AssociatedTasks'].fillna('Other')\n",
    "    df['Area'] = df['Area'].fillna('Other')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column for each data characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_characteristics_columns(df): \n",
    "    # force data set characteristics column to string data type\n",
    "    df['DataSetCharacteristics'] = df['DataSetCharacteristics'].astype(str)\n",
    "\n",
    "    # create a column for each data characteristic value is 1 if true, 0 if false\n",
    "    df['multivariate_data'] = df['DataSetCharacteristics'].str.contains('Multivariate').astype(int)\n",
    "    df['time_series_data'] = df['DataSetCharacteristics'].str.contains('Time-Series').astype(int)\n",
    "    df['data_generator_data'] = df['DataSetCharacteristics'].str.contains('Data-Generator').astype(int)\n",
    "    df['domain_theory_data'] = df['DataSetCharacteristics'].str.contains('Domain-Theory').astype(int)\n",
    "    df['image_data'] = df['DataSetCharacteristics'].str.contains('image').astype(int)\n",
    "    df['relational_data'] = df['DataSetCharacteristics'].str.contains('Relational').astype(int)\n",
    "    df['sequential_data'] = df['DataSetCharacteristics'].str.contains('Sequential').astype(int)\n",
    "    df['spatial_data'] = df['DataSetCharacteristics'].str.contains('Spatial').astype(int)\n",
    "    df['univariate_data'] = df['DataSetCharacteristics'].str.contains('Univariate').astype(int)\n",
    "    df['spatio_temporal_data'] = df['DataSetCharacteristics'].str.contains('Spatio-temporal').astype(int)\n",
    "    df['text_data'] = df['DataSetCharacteristics'].str.contains('Text').astype(int)\n",
    "    df['transactional_data'] = df['DataSetCharacteristics'].str.contains('Transactional').astype(int)\n",
    "\n",
    "    #delete original data set characteristics column\n",
    "    del df['DataSetCharacteristics']\n",
    "\n",
    "    #define function to add values from all data characteristic columns in each row\n",
    "\n",
    "    num_characteristics = lambda row: (row.multivariate_data + row.time_series_data + row.data_generator_data +\n",
    "                                    row.domain_theory_data + row.image_data + row.relational_data + row.sequential_data +\n",
    "                                    row.spatial_data + row.univariate_data + row.spatio_temporal_data)\n",
    "\n",
    "    #create column to count number of data characteristics and apply lambda\n",
    "    df['num_data_characteristics'] = df.apply(num_characteristics, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column for each attribute characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attribute_columns(df): \n",
    "    # force attribute characteristics column to string data type\n",
    "    df['AttributeCharacteristics'] = df['AttributeCharacteristics'].astype(str)\n",
    "\n",
    "    #ceate a column for each attribute characterstic, value is 1 if true, 0 if false\n",
    "    df['AttributeCharacteristics'] = df['AttributeCharacteristics'].astype(str)\n",
    "    df['categorical_attributes'] = df['AttributeCharacteristics'].str.contains('Categorical').astype(int)\n",
    "    df['real_attributes'] = df['AttributeCharacteristics'].str.contains('Real').astype(int)\n",
    "    df['integer_attributes'] = df['AttributeCharacteristics'].str.contains('Integer').astype(int)\n",
    "    df['integer_attributes'] = df['AttributeCharacteristics'].str.contains('Integer').astype(int)\n",
    "    df['no_listed_attributes'] = df['AttributeCharacteristics'].str.contains('Other').astype(int)\n",
    "\n",
    "    #delete original attribute characteristics column\n",
    "    del df['AttributeCharacteristics']\n",
    "\n",
    "    #define function to add values from all attribute characteristic columns in each row\n",
    "    num_attribute_types = lambda row: (row.categorical_attributes + row.real_attributes + row.integer_attributes +\n",
    "                                        row.no_listed_attributes)\n",
    "\n",
    "    #create column to count number of attribute characteristics and apply lambda\n",
    "    df['num_attribute_characteristics'] = df.apply(num_attribute_types, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column for each Associated Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tasks_columns(df): \n",
    "    # force associated task column to string data type\n",
    "    df['AssociatedTasks'] = df['AssociatedTasks'].astype(str)\n",
    "\n",
    "    #ceate a column for each associated task, value is 1 if true, 0 if false\n",
    "    df['causal_discover_task'] = df['AssociatedTasks'].str.contains('Causal-Discovery').astype(int)\n",
    "    df['classification_task'] = df['AssociatedTasks'].str.contains('Classification').astype(int)\n",
    "    df['regression_task'] = df['AssociatedTasks'].str.contains('Regression').astype(int)\n",
    "    df['function_learning_task'] = df['AssociatedTasks'].str.contains('Function-Learning').astype(int)\n",
    "    df['recomendation_task'] = df['AssociatedTasks'].str.contains('Recommendation' or\n",
    "                                                                                        'Recommender-Systems').astype(int)\n",
    "    df['description_task'] = df['AssociatedTasks'].str.contains('Description').astype(int)\n",
    "    df['relational_learning_task'] = df['AssociatedTasks'].str.contains('Relational-Learning').astype(int)\n",
    "    df['no_given_task'] = df['AssociatedTasks'].str.contains('Other').astype(int)\n",
    "    df['clustering_task'] = df['AssociatedTasks'].str.contains('Clustering').astype(int)\n",
    "\n",
    "    #delete original associated task column\n",
    "    del df['AssociatedTasks']\n",
    "\n",
    "    #define function to add values from all associated task columns in each row\n",
    "    num_associated_tasks = lambda row: (row.causal_discover_task + row.classification_task + row.regression_task +\n",
    "                                        row.function_learning_task + row.recomendation_task + \n",
    "                                        row.description_task +  row.relational_learning_task)\n",
    "\n",
    "    #create column to count number of associated tasks and apply lambda\n",
    "    df['num_associated_tasks'] = df.apply(num_associated_tasks, axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert date donated to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas._libs.tslibs.timestamps.Timestamp"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "def convert_to_datetime(df):\n",
    "    df['DateDonated'] = pd.to_datetime(df['DateDonated'])\n",
    "    return df\n",
    "type(pre_cleaned_df['DateDonated'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all remaining NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_na_drop(df):\n",
    "    df1 = df.dropna()\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandata = fillna(pre_cleaned_df)\n",
    "cleandata = create_characteristics_columns(cleandata)\n",
    "cleandata = create_attribute_columns(cleandata)\n",
    "cleandata = create_tasks_columns(cleandata)\n",
    "cleandata = convert_to_datetime(cleandata)\n",
    "cleanest_data = final_na_drop(cleandata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleanest_data.to_csv('cleanest_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Country Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_univ_city(dfsrc, uniquedf):\n",
    "    \"\"\" Extract University, City, and Country values from \"Source\"\n",
    "    column of original UC Irvine dataset by using a lookup list\n",
    "    developed by hand to solve this dataset problem. \"\"\"\n",
    "\n",
    "    def get_Univ_Loc_match(rowvals):\n",
    "        \"\"\" Have to use sub-function to run function on\n",
    "        each row of a dataframe using apply. \"\"\"\n",
    "\n",
    "        # If encounters a number, go ahead and return nothing\n",
    "        if isinstance(rowvals, float) or isinstance(rowvals, int):\n",
    "            return \"\"\n",
    "        #Inherit the lookup list from parent function\n",
    "        tester = uniquedf.copy() \n",
    "        #Create boolean column to check if each lookup value exists \n",
    "        # in the column being checked, ensure all texts are stripped\n",
    "        # and all text is lowercase to ensure matches work.\n",
    "        tester['boole'] = tester['LookupVal'].apply(lambda x: x.strip().lower() in rowvals.strip().lower())\n",
    "        tester = tester[tester['boole'] == True] #return only rows with match\n",
    "        # Convert matched rows to a list which can be saved in the column\n",
    "        tester = tester[['University', 'City', 'Country', 'CODE']].values.tolist()\n",
    "        \n",
    "        # If nothing results from lookups, return nothing\n",
    "        if not tester:\n",
    "            return \"\"\n",
    "        else:\n",
    "            # Otherwise return a unique list of Countries. Multiple\n",
    "            # Lookups per value means could be multiple hits for 1 match\n",
    "            tester = [list(x) for x in set(tuple(x) for x in tester)]\n",
    "            return tester\n",
    "\n",
    "    dfsrc['source_institution_places'] = dfsrc['Source'].apply(get_Univ_Loc_match)\n",
    "\n",
    "    return dfsrc\n",
    "\n",
    "def create_lookup_list(dfsrc):\n",
    "    \"\"\" Original starting place for creating the lookup list.\n",
    "    At first I was collecting data by hand, which turned into making this. \"\"\"\n",
    "    df1 = dfsrc[['University1', 'Location1']]\n",
    "    df2 = dfsrc[['University2', 'Location2']]\n",
    "    df3 = dfsrc[['University3', 'Location3']]\n",
    "    renamecols = ['University', 'Location']\n",
    "\n",
    "    for i,idf in enumerate([df1, df2, df3]):\n",
    "        idf.columns = renamecols\n",
    "        if i != 1:\n",
    "            df1.append(idf, ignore_index=True, sort=False)\n",
    "    \n",
    "    unq = df1.groupby(renamecols).size().reset_index()\n",
    "    return unq\n",
    "\n",
    "def add_locations(updatemedf):\n",
    "    \"\"\" Add locations data to a dataframe \"\"\"\n",
    "    # List of unique identifiers to search in text\n",
    "    uniquedf = pd.read_csv('uniquelist.csv', encoding=\"latin-1\")\n",
    "\n",
    "    # Use lookup list to find text and look for institution matches, \n",
    "    # once found append unique list of matching institution lookups\n",
    "    df = add_univ_city(updatemedf, uniquedf)\n",
    "    # Output this to a file for checking and adding more values as needed\n",
    "    return df\n",
    "\n",
    "def join_dfs(cols_source_df, col_to_add, add_to_df, join_on_col):\n",
    "    \"\"\" Join two dataframes together, resulting in 2 columns added. \"\"\"\n",
    "    List_cols_to_add = [col_to_add, join_on_col]\n",
    "    cols_source_df = cols_source_df[List_cols_to_add]\n",
    "    add_to_df = add_to_df.join(cols_source_df.set_index(join_on_col), on=join_on_col)\n",
    "    return add_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "--- 0:00:02.952101 seconds ---\n"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # 1. src dataset to build on -> dataframe\n",
    "\n",
    "    src_df = pd.read_csv('cleanest_data.csv', encoding=\"latin-1\")\n",
    "\n",
    "    add_loc_df = pd.read_csv('dataset_add_Univ_City.csv', encoding=\"latin-1\")\n",
    "\n",
    "    # # 2. add lookup-text-search institution / location values to src\n",
    "    add_loc_df = add_locations(add_loc_df)\n",
    "\n",
    "    # 2.5 then add those looked up values to the real cleaned dataset\n",
    "    src_df = join_dfs(add_loc_df, \"source_institution_places\",src_df, \"NumberofWebHits\")\n",
    "    src_df['YearAdded'] = src_df['DateDonated'].apply(lambda x: pd.to_datetime(x, infer_datetime_format=True).year)\n",
    "    src_df['DatasetAge'] = src_df['YearAdded'].apply(lambda x: 2020-x)\n",
    "\n",
    "    def calc_num_cells(x):\n",
    "        out = x['NumberofInstances'] * x['NumberofAttributes']\n",
    "        return out\n",
    "    src_df['DatapointCount'] = src_df.apply(calc_num_cells, axis=1)\n",
    "\n",
    "    # # 3. export finished df to file for easy access\n",
    "    src_df.to_csv('cleanest_data_KMaugmented.csv', encoding=\"latin-1\", index=False)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_df = pd.read_csv('cleanest_data.csv', encoding=\"latin-1\")\n",
    "\n",
    "add_loc_df = pd.read_csv('dataset_add_Univ_City.csv', encoding=\"latin-1\")\n",
    "\n",
    "# # 2. add lookup-text-search institution / location values to src\n",
    "add_loc_df = add_locations(add_loc_df)\n",
    "\n",
    "# 2.5 then add those looked up values to the real cleaned dataset\n",
    "src_df = join_dfs(add_loc_df, \"source_institution_places\",src_df, \"NumberofWebHits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['University of Genoa', 'Genoa', 'Italy', 'ITA'],\n ['Universitat Politecnica de Catalunya', 'Barcelona', 'Spain', 'ESP'],\n ['Universite degli Studi di Genova', 'Genoa', 'Italy', 'ITA']]"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "src_df['source_institution_places'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}